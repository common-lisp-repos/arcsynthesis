adding vectors  v1 + v2 = v3
-positions: 
 first go v1 then go v2 then you arrive at v3
-direction
 v3 is the direction between v1's origin and v2's tip

negation:
-position: backwards
-direction: 180-degree turn
 
subtracting: like addition but v2 is negated before adding it to v1
-position:
 first go v1 then go v2 backwards
-direction
 very interesting it seems to be a 90-degree angle of vector addition ~~**

multiplication:
no geometric equivalent. Just component-wise multiplication.

vector-scalar multiplication:
results in longer/shorter vector

length: sqrt(x^2,y^2,z^2...) of any dimension

unit-vector: ^ atop vector name   v^ = v1 * 1/length(v1)

--------------------------------------------------------------------------------
image = 2d-array of pixels
model,mesh,geometry = object made of many triangles

triangles -> [rasterization] -> (2d) image
              ^^^^accessed by OpenGL
OpenGL is an API for accessing the hardware-based rasterizer

Rasterization:
1.triangle goes in
2.is it even in "clip space"? (region of the world we want to render)
3.It resides in "clip space" and has "clip coordinates" (always 4:x,y,z,w)
  Each vertex of a triangle in clip space has its own clip space cube of range [-w,w]
4.Triangles outside clip space aren't operated upon further on
  Triangles partially in clip space will be broken down into smaller triangles which
  are entirely within clip space. This process is called clipping, hence the name clip
  space. "noclip" or "clip through the ground" are then misnomer regarding cheats/bugs
5.clip space is transformed into "normalized device coordinates": x,y,z is divided by w!
  Now the range of x,y,z are [-1,1]
6.window transformation: coordinates relative to window that "OpenGL" is running within
  Z is [0,1] so a game world must be mapped depth-wise to a scale from 0 to 1 ??
7.scan conversion:
  which window-pixels overlay with the triangle? -> creates blocky triangle!
  "sample": are of pixel determining if triangle overlapping said pixel will produce a
  "fragment"
  Guarantees: a) shared edges between triangles will never have sample gaps
              b) same input triangle will always result in the same pixel-art fragments
  Fragments still have a z value, not only x,y!!!
8.Fragment processing:
  transform fragment into color + depth value
  note of understanding: fragments from different triangle can overlap UNDERSTAND??
  Fragments from one triangle must be all processed before another triangle is processed
  >>Direct3D calls this step: "pixel processing" or "pixel shading"... MISNOMER!!
9.Fragment Writing: These mosaics of depth fragments which can overlap with other triangle
  fragments are written to the destination image
  It may combine its color and depth with colors that are already in the image!

Shaders, programs run on GPU.
shader-stages: at certain points in the rendering(rasterization)-process, we can program
arbitrary algorithms to create a specific visual effect.
Shader-stage examples: fragment writing, putting triangle vertex into clip space.
>>shaders run on the actual rendering hardware!! -> no CPU time wasted!!!!1
  therefore downside: they have certain limits CPU code wouldn't have

GLSL, the language used to write shaders.

--------------------------------------------------------------------------------
OpenGL API:
bunch of typedefs:
      (renamed variable types, but still just the basic int, float etc:
      typedef int points;
      points current_score; // points is like int; this line is like: "int current_score"
      show_Score(points my-points); invocation: show_Score(current_score)
      equivalent to:
      int current_score
      EXAMPLES: GLint, GLfloat defined to have a specific bit depth

enumerators:
  constants of specific names. They serve as identifiers but are in in C just constants
  of type int mostly and maybe exclusively.
  Outside OpenGL example: enumerator: Boolean, value: true/false representation:
  :true :false

functions: you know who you are!

Complex data types are never directly exposed in OpenGL, e.g. structs in C++
struct Object { int count; float opacity; char *name; };
/* create storage: */ Object newObject;
/* Put data into object: */ newObject.count = 5; newObject.opacity = 0.4f; ....

In OpenGL this would look like this:
// create storage for the object:
GLuint objectName;
glGenObject(1, &objectName)
// Put data into the object:
glBindObject(GL_MODIFY, objectName);  //Cooool !!
glObjectParameteri(GL_MODIFY, GL_OBJECT_COUNT, 5);     
glObjectParameterf(GL_MODIFY, GL_OBJECT_OPACITY, 0.4f); 
glObjectParameters(GL_MODIFY, GL_OBJECT_NAME, "Some String");  

NONE OF THESE ARE ACTUAL OPENGL COMMANDS, they're just here to give you an idea!!!
and it's a cool idea alright!! ^_^

OpenGL owns the storage for all OpenGL objects, so the user must access those objects
through unsigned integers (the GLuint) (teehee!)

Objects are created by functions of the form: glGen* (* is the type of the objects)
the first parameter to glGen* is the number of objects to create! and the second a
GLuint* array that receives the newly created object names.

modify Objects:
first they need to be bound to a specific location in the context (called: targets) all
objects have a _list of valid targets_

fields in the Object:
enumerators GL_OBJECT_* all name fields in the object that can be set!!

Once an objects is generated, bound to a target, we can set parameter within the object
using the glObjectParameter family (like in the above example!)
Since OpenGL is a C API, it has to name each differently typed variation differently:
"Parameteri for integer parameters, "Parameterf for float ... etc.

--------------------------------------------------------------------------------
OpenGL context:
the API is defined as a state machine, hence we either read states , change state
or render the scene using the state info.
If you were to create  multiple windows for rendering, each would have its own
OpenGL context (get it?)
--------------------------------------------------------------------------------
Referring to objects needs GLuint handlers, as mentions above. 0 is the equivalent
of a NULL pointer.
Hence binding an object to 0 means unbinding the currently bound object!
--------------------------------------------------------------------------------
buffer objects, data storages that OpenGL can "see", we fill them with our data
(triangle vertex, picture on hdd..)
--------------------------------------------------------------------------------
Shaders are executed over a set of inputs and converts them into a set of outputs
Example vertex shader:

#version 330  //must always state version
              // 1. the inputs:
              // variable position of type: vec4 (4-dim vector of float values);
              // layout will be explained later
layout(location = 0) in vec4 position;
void main()   // shaders execution always start with main() 
{
              // assign the BUILT-IN variable gl_position the value
              // ALL built-in variables start with "gl_"!!
    gl_Position = position;
}

This built-in gl_Position is defined as:

out vec4 gl_Position

The minimum a vertex shader must do is create a clip-space position. That is what
gl_Position is: the clip-space position of the vertex
--------------------------------------------------------------------------------
But where does "positions", the input, gets its data from. Inputs to a vertex
shader are called "vertex attributes"

Vertex attributes, are the inputs to a shader. They have a index location called
"attribute index". The vertex attribute was defined with:
layout(location = 0) in vec4 position;
^^^ the layout part assigns the attribute index 0 to position!!
It was first hardcoded here:
(gl:enable-vertex-attrib-array 0) !
And our "format-of-buffer-object-describing-function" deals with this exact index:
(gl:vertex-attrib-pointer <index> 4 :float :false 0 0) ; namely 0 !!!
^^^^In effect it describes where the data for an attribute comes from

Interesting OpenGL-state-machine fact:
The order in which gl:enable-vertex-attrib-array and gl:vertex-attrib-pointer is
called doesn't matter. All that matters is that they are called before rendering
takes place (gl:draw-arrays..) because we just set state in OpenGL!!
--------------------------------------------------------------------------------
Fragment Shader,
the inputs are: window-space xyz position of fragments and also user-defined data:

#version 330

out vec4 outputColor; //this time its an "out" (output)!
void main
{
    outputColor = vec4(1.0f, 1.0f, 1.0f, 1.0f);
}
--------------------------------------------------------------------------------
A SHADER STRING gets compiled into a shader object; analogous to C object file
(when a c program is compiled a object file is created)
One or more SHADER OBJECT can be linked into a: PROGRAM OBJECT.

These program objects contain aaaal the shaders to be used for rendering!
--------------------------------------------------------------------------------
a Float consists of 4 bytes
in GLSL a vec4 is a sequence of 4 floats (vec4 = size 16 bytes)
--------------------------------------------------------------------------------
We try to "move" a triangle,
we change the data of inside our gl:gl-array *vertex-positions* by using
glBufferSubData(). The difference to glBufferData() is that the latter ALLOCATES
memory specifically of a certain size.
glBufferSubData() transfers data to ALREADY existing memory!!

The C pro should think of glBufferData as combination of malloc + memcpy
while glBufferSubData is just memcpy ;> TODO

AS EXPECTED/(interesting!!!)
the ominous parameter to glBufferData() :static-draw tells OpenGL that we only
intend to set the data in this buffer ONCE.
Opposite: :stream-draw. API wise THEY MEAN NOTHING though they're just hints for
<<<<<<< HEAD
the implementors where to work on performance wise!!!
=======
the implementors where to work on performance wise!!!
--------------------------------------------------------------------------------
uniforms,
unlike 'in' variables, these shader variables do not change every time the
vertex shader is called. Instead they change only between rendering call executions!
And even then they only change if they're set explicitly!!

Also 'in' variables of vertex shaders receive their input via :array-buffers etc.
uniforms, however, receive it by directly setting them via program objects
Remember: program object = linked shader objects
Unlike 'in' variables, we can't set the location ourselves, we NEED TO QUERY it:
offsetLocation = glGetUniformLocation(theProgram, "<varName-in-shader-program>"
                                                   ^^^ wa-wa-wi-wa
This gives us the uniform location for a GIVEN <varName..>!!!

We can then set the value using (gl:uniform-2f <varName..> value1 value2)
provided in our glsl we used a vec2 uniform ^  ;D
that is in our *.glsl file:
    uniform vec2 offset --------------------------------------.
corresponding code:                                           v
    (setf offset-location (gl:get-uniform-location program "offset")
    ...                                              |
    (gl:use-program program) <------------------------
    ...
    (gl:uniform-2f offset-location 88.0 99.0)
--------------------------------------------------------------------------------
Fragment shaders though store the position of a fragment, cannot change it!
they can only change the color of it.
--------------------------------------------------------------------------------
Globals in shaders!
As you will remember "uniforms" are shared between all shader stages and GLSL
won't even allow to use their name for differently typed variables.

Other globals WHICH ARE SHARED BETWEEN SHADER STAGES include:
in, out and uniforms
--------------------------------------------------------------------------------
CPU vs. GPU performance,
Our sin()/cos() offset computation is rather fascinating. Our initial CPU process
computes the offsets once and iterate over all the vertices to apply them, whereas
the shader computes the SAME OFFSET every shader call, which must be called the number
of vertices in the :array-buffer!

We need therefore to weigh the operations performed in a shader and decide if computing
the same computation times the vertices in a shader vs. computing them once and on the CPU.

Indeed, performance-wise usually the common practice solution is to compute the
value on the CPU and pass it as a uniform to the shader.
--------------------------------------------------------------------------------
gl:cull-face, don't render triangles classified as :back or :front for performance
boost... "cull 'em out of the rendering!"

Interesting: what classifies as back, depends on the vertex data. The outer faces
of any object should have such property when viewed from outside!!!
--------------------------------------------------------------------------------
"projection",
is a term in the rendering pipeline, which describes a way to transform a world from
one dimensionality to another.

"orthographic projection",
e.g.: project a 2d world perpendicular to a 1d line

"perspective projection",
how human eyes model the world as a picture in the mind.
In OpenGL our eye's view field is within the:

Frustum,
a pyramid with a chopped of head. Instead of a cone, as the eye would use it, in
OpenGL we use a frustum to project the 3d world on a "rectangular" image.

Perspective Computation:
the following vectors:
R = projection on "plane of projection"
E = distance from Eye to perpendicular "plane of projection"
Pz= distance from Eye to perpendicular plane containing vertex to be projected
P = to be projected vertex (this value is the one stored in :array-buffer for example)

then:
R = (E/R)*P

Imagine P being on a straight line with Eye. Then E/R*P would of course result
in P having the length of E to the projection plane. Now as we move P left and right
(perpendicular to the projection plane!!)
we just create a diagonal line from P to E intersecting the plane of projection left
and right. It should be possible now to imagine that the ratio of E/P multiplied with
P will result in that projection!!
Which is in effect not a resource heavy computation at all!

Which leaves one loose loop: how to attain these perpendidularities!
TODO: (I think dot product and the likes are designed for this)

Now comes the super duper awesome part:
R =  P * (E/Pz)
can be expressed as a division (MULTIPLICATION BY THE RECIPROCAL)
R = P / (Pz/E)

and now guess what? NORMALIZED DEVICE COORDINATES?? W??
yep, (x,y,z, w) <- this w when using NORMA..lized device coordinates does this:
divide each component x,y,z by w and

NOW ALL WE NEED TO DO IS SET W to this in EACH vertex:

       w = E/Pz

and normalized device coordinates will yield a PERSPECTIVE PROJECTION!!!!!!

E is a constant, only Pz can be an unknown if viewed from just any angle
but I bet there is some mathematical trick to yield proper perpedicularities!

========================================================================================
#     This conversion from clip-space to normalized device coordinate is called        #
# "PERSPECTIVE DIVIDE" exactly because it is usually used for perspective projection!! #
========================================================================================

Up until now our vertex shaders received clip space coordinates and output clip space
coordinates as well hence:
gl_Positions = positions // a no brainer for the VERTEX shader

Now we want to change the inputs of our vertex shader, probably so that it will get
a proper W value for NDC's perspective divide.

We therefore define a new space for positions which we leisurely call: CAMERA SPACE

projection plane,
part of the Frustum, on which all the projection will take place. It extends in
[-1,1] on x and y and is fixed at Z = -1. The projection will be from vertices
from the -Z direction!! We assume the "eye" to be at the origin
[0,0,0].

aaaanywho:
The frustum is already finitely bound in the x and y directions now Z needs a
finite boundary in camera space, we call them

camera zFar,
the maximum distance a vertex can be before it is considered "not in view"

camera zNear,
the minimum distance from the eye, before we decide it's not in view.

Those are the definitions for camera space, later we will reuse these words
connoting other meanings :I
--------------------------------------------------------------------------------
swizzle selection (GLSL syntax),
access multiple components of a vector and return them as a proper dimensional
vertex:
  vertex.zy => returns: vec2(vertex.z, vertex.y)

Note how you can dictate the order (the .zy)! Also vector.zzx is possible
(two times the same value) etc.
You may not select more than 4 values (vec4 is the limit!).
  
This swizzle selection is very efficient, arcsynthesis even states it has probably
has NO performance penalties.
--------------------------------------------------------------------------------
The Matrix has You,
though a vector * matrix multiplication may perform 16 float multiplications and
12 float additions it is still very efficient: graphics hardware is designed to make
these operations very fast!

Also: each of these multiplications can be done INDEPENDENTLY which opens the doors
for parallel processing, which is exactly the kind of thing graphics hardware does
fast (SANIC, he's the fastest thing aliiiiiiiiiiii-)
Also each addition operations are partially independent. Ultimately each
vector-matrix multiplication usually generates only 4 instructions in the GPU's
machine language!

(gl:uniform-matrix ...) very useful function!
--------------------------------------------------------------------------------
VAO,
up until now we had to draw using the following steps: generate buffer object handle,
use it to bind it to a target in OpenGL, use it to put data into OpenGL, specify
the format it's in, extract an attribute index from it and finally use it in a
shader and gl:draw* the data it represents.
the VERTEX ARRAY OBJECT (VAO) is here to alleviate us of this burden!

It essentially stores all the states needed, we set them implicitly, so our code
looks cryptic and voodoo to the uninitiated and also we use real voodoo code
ourselves because of API cruft! Aaaaanywho, it gets complicated but we still
can get through it using the state-machine metaphor.

The end-product will be a VAO-object which we will (gl:bind-vertex-array *vao-obj*)
and call (gl:draw-* ..) then we will set the next VAO-obj to draw a second object.
The abstraction of "drawing an object" is explained by Mr. McKesson as:
"An object, in terms of what you draw, can be considered the results of a single
drawing call."
That drawing call is what we do after we (gl:bind-vertex-array *vao-obj*) to the
context, as just described.
--------------------------------------------------------------------------------
Check this out:
Mr McKesson suggests in a pseudo code for gl:draw-arrays, that they loop over some
given range using indices within it to call:

  VertexShader(position-attribute-array[some-index]

(!) We're calling some "VertexShader" function implicitly!! (!)
--------------------------------------------------------------------------------
ARRAY DRAWING,
use the array as a movie-tape and run it sequentially to draw stuff.


ELEMENT ARRAY (also index array),
cheat sheet from a periodic table and random access what element you want to draw.
All you have to do is use your INDEX finger to get at the elements!

element-arrays are stored in buffer objects and need to be bound to the
:element-array-buffer target!
The functions gl:buffer-data gl:buffer-sub-data will operate on it, hence.

Now this "random access" of element buffer drawing needs to be put in the desired
sequence of "random access" which are stored in the element-array! Then they
are read and there elements are indices to position/color-attrib-arrays!!
--------------------------------------------------------------------------------
Notes on C++ feature: #define directives
They are preprocessor commands, before the .cpp file is compiled any #-preceding
line is interpreted as a preparation command before actual compilation takes place
it is simple:
#define FOO 22
int 1 + FOO;

will first translate the file to:
int 1 + 22; then compile it

so it's just textual replacement, and as I've read it is considered a "macro" and an
issue which involves the association/confusion of Lisp macro and C++ #define directive
macro.

An Arcsynthesis example:
#define ARRAY_COUNT( array ) (sizeof( array ) / (sizeof( array[0] )
* (sizeof( array ) != sizeof(void*) || sizeof( array[0] ) <= sizeof(void*))))
// note the above must be inline, newline should actually involve a '\' character

ARRAY_COUNT(foo)

translates to
( foo ) (sizeof( foo ) / (sizeof( foo[0] ) * (sizeof( foo ) != sizeof(void*) ||
sizeof( foo[0] ) <= sizeof(void*))))
--------------------------------------------------------------------------------
hidden surface elimination,
What anticipated all along: The rasterizer just draws triangles on the screen:
The last thing drawn, regardless of being  geometrically behind or inside an object,
is always drawn on top!
The rasterizer is "stupid" it knows nothing other than drawing triangles, but
it does it very efficiently.

depth sorting,
the naive solution, like a bubblesort, is the first solution that comes to mind
and it is inefficient and even incomplete(!):
We sort objects from closest to farthest from the screen, then render them back
to front, hence no overlapping of stuff actually in front can happen, right?

Just imagine two triangles intersecting, or three overlapping each other
domino-style we have no definite "front" candidate.

The next naive solution naturally builds on the idea, and is still quite armchair
feasible: we do the same thing with individual fragments: after all, there
are no intersecting fragments. The only problem is when multiple share the same
z-value on a huge contiguous array of pixels on the screen. Then we get unique
problems like: "z-fighting".

tagging fragments,
associate with every fragment its window-space z-value, compare which value is
the smallest, hence closes to eye, and only draw those fragments! We store those
z-values in a:

depth buffer,
also called the z-buffer, since it stores z-values!

Interesting: Colors output from a fragment shader are stored in the:
color image buffer, which together with the z-buffer is CREATED BY OPENGL
automatically when it is initialized already!!

depth test,
Stop the fragment from being written to the screen if it fails this test.
We enable it with (gl:enable :depth-test), him so the rasterizer is smarter then
we gave it credit?
Also we need to call gl:depth-func to set the relation of the depth test. This
relation can be :always (always write the fragment), :never now it gets interesting:
:less, :greater :lequal :gequal :equal :notequal
It's always the same (read left to right relation):
          1.                      2.                        3.
    fragment depth <relation-set-in-gl:depth-func> depth-buffer value

e.g.:  0.2  :less 0.5  => fragment gets written!

gl:depth-mask causes the fragments depth to be written to the depth buffer

Finally, clearing the depth values is the same as clearing color values in the
corresponding buffers: we set the bits, :depth-buffer-bit, and gl:clear them
(gl:clear-color x y z w) 
(gl:clear-depth 1.0)
(gl:clear :color-buffer-bit :depth-buffer-bit)
--------------------------------------------------------------------------------
Translation,
Use the origin of one space, and place it somewhere in another, the vertices will
follow relative to the new origin..
or "how to offset -- vertex graphics programming style". We will be using matrices
now because of advantages, yet to be explained. So we need a "offset" matrix, that
once the vertex vector multiplies with it will offset some of its values.

The identity matrix,
will help, it is like 1, in multiplication, anything multiplied with it will return
whatever was multiplied (identity element). math-combination(identity-element, xyz) = xyz
Here's how it looks like:
 [1, 0, 0, 0 
  0, 1, 0, 0
  0, 0, 1, 0
  0, 0, 0, 1]
Now remember the special properties of OpenGL matrices: The 'w'-column is 1.0 by default,
so if we want to offset a vector, we just need to put in the offset in there

 [1, 0, 0, x      [2    2 + 0 + 0 + x  =  [2+x
  0, 1, 0, y   X   4  = 0 + 4 + 0 + y  =   4+y
  0, 0, 1, z       3    0 + 0 + 3 + z  =   3+z
  0, 0, 0, 1]      1]   0 + 0 + 0 + 1  =    1 ]

Now I hope this visual doesn't confuse you into thinking matrix multiplication works
horizontally! But multiplying with a identity-matrix sure makes it look like it!
--------------------------------------------------------------------------------
Finally the grand Matrix mystery is solved!!
The matrix is a numerical representation of a coordinate system! Its rows
left to right are the basis vectors of the coordinate system. The final row
is the offset applied to it all, the origin.
Taking the identity matrix:
 [1, 0, 0, 0 
  0, 1, 0, 0
  0, 0, 1, 0
  0, 0, 0, 1]
We have the usual coordinate system with x right [1 0 0 0] y up [0 1 0 0]
changing x to [2 0 0 0] hence means that when we put this coordinate system in
another (namely through transformation matrix multiplication with all vertices)
all the x-values will be stretched along this basis vector by 2!
Switching the basis vectors:
 [0, 1, 0, 0 
  1, 0, 0, 0
  0, 0, 1, 0
  0, 0, 0, 1]
Results in the intuitive observation, the object is rotated by 90-degree in the
output coordinate system.

This also means that we can distort our objects like crazy!

And most importantly:
The problem of rotation along an axis is reduced to moving the basis vectors
in a circular fashion!!
Rotation along the z-axis can be done using:
 [cos(rad), -sin(rad), 0, 0 
  sin(rad), cos(rad)   0, 0
     0,        0,      1, 0
     0,        0,      0, 1]
We want to move the x and y basis vectors. If we don't move them: rad = 0
we get the identity matrix as cos(0) = 1; sin(0) = 0
"slowly" rotating will decrease e.g. cos(0.01) = 0.9 and increase sin
We can make the mental image of the x-basis-vector to increase in its y value
(look at the sin(rad) placement in the rotation matrix, it is at the 'y' for
the x-basis vector) and decrease in its x value (...).
And again the magic of the oscillating sinus and cosines functions enchant us
as the inputs venture towards infinity the bounds persist: [-1, 1] which the rotation
matrix captures by rotating our basis vectors!
Though we want to restrain the inputs by taking the modulo of some global elapsed
time and an arbitrary loop duration.

We plant origins in another space and let it unfold along its basis vectors.
--------------------------------------------------------------------------------
Why Matrices,
The main reason to use matrices is that they can store a complex transformation,
and all it takes is for a vertex to be multiplied with it once for it to work.
This is due to matrices, though not commutative are distributive:

A scale Matrix can be combined with a translation matrix (as we did in the first
place) and even with a rotation matrix, into a single matrix. This amalgam matrix
can be computed by the CPU and then uniform-handled to the shader and be used by
all vertices.

commutative:
while 2*3*4 = 4*2*3  for matrices this is NOT true
distributive:
while 2*3*4 = 2*(3*4) = (2*3)*4 IS TRUE for matrices, hence:
translation-matrix * scale-matrix * vertex
is the same as
CPU-compute(translation-matrix * scale-matrix) = amalgam-matrix
shader-upload-and-compute(amalgam-matrix * vertex)!

As the order of matrices are concerned we should first translate the point
then scale or rotate. Scales should happen before rotation or else the scaling will
be relative to the rotation axis! What one normally wants is to scale relative
to the model space (given vertices).

Hierarchical model,
very awesome stuff:
 We move points by translating, scaling and rotating then we draw intermediates. If we
 now continue to transform further we can draw again and we yield object relative to
 each other because transformation is passing vertices from one coordinate system to
 another, this also means that the higher coordinate system passes on its orientation
 to the one represented "within" it. We get results like spinning one axis spins every
 object drawn within child transformation coordinate systems.
Example given is the armature simulation, rotating the main arm rotates every object
build upon the transform that spawned the main arm. We hereby open a door to render
intricate objects previously perhaps poorly understood and hence daunting to do.

The daunting task remains to imbue hierarchical models in collision logic!!!

On scaling hierarchical models,
if the main object used to draw all resulting objects from successive transformations
has its vertices in different quadrants of a coordinate system we get a very convenient
effect: scaling now can be done intuitively as vertices being multiplied will move
away from the origin and relative to each other at the same rate, or attract each
other.
This is very useful also to incrementally design hierarchical models as we can
WATCH as scaling just blows the object apart or shrinks it IN PLACE.

Translation, scaling in hierarchical models,
is nearly never problematic as it has an identity matrix in its first 3 rows/col
and intuitively only offsets given points.
But a scaling matrix, in contrast, would distort even translation matrix, as it
would change the 'w'-column due to its x, y, z non 1 values!

--------------------------------------------------------------------------------
Again on col-major, row-major arc/glsl syntax,
  col-major, row-major only matter in memory representation (col-major: columns are
  contiguous). Anyway the syntax used by arc is bit confusing: mat[3] reads the 3rd column
  the rows however contain each a vectors x,y,z,w in order. So mat[..] will read a vector
  of purely x,x,x,x or in the case of mat[3] w,w,w,w. This is inferred by the implementation
  of translation (sets mat[3] and scaling).
  The confusing part is this:
  mat[3].y reads the "2nd 'w'" because mat[3] = w,w,w,w e.g. (1.0 2.0 3.0 4.0) are all 'w' of
  the corresponding vectors in a row. So mat[3].y reads 2.0       ^y even though all are 'w'
  
The core of the problem is that we want to read with our eyes row-major but in memory
represent with col-major (as is what OpenGL wants)
For example one very pernicious bug I've purged was the creation of a matrix.
Simplification:
2x2 matrix:
(list 1 3
      2 4)
printing it like this the intent is to fool the reader into it being col-major, but
lists are read in sequence hence the matrix siphoning these values will look like so:
[1 2
 3 4]
Where the printed representation is intuitive (columns are really vertical, rows horizontal).
Therefore gl:uniform-matrix' defaulting to transposing its input can be quite handy when
we want to read in row-major but have, because of OpenGL, deal in the background with
col-major.
hence we can pass the (list 1 3             (list 1 3 2 4) :I
                            2 4) and transpose it upon setting the uniform.

But when we build matrices with glm:matrix(...), which we explicitly set by
functions labeled column/row etc. We don't want to transpose them on input.

That's where the 2nd cl-opengl problem comes in:
(gl:uniform-matrix *mat-unif* 4 (vector *mat4*) :false)
still transposes the input matrix even though    ^^^^^ is :false!!!!
Well... we can't use the GL_FALSE => :false approach on this one, it is just NIL!

The right call to set the uniform unscathed of them rude transposifications is:
(gl:uniform-matrix *mat-unif* 4 (vector *mat4*) NIL)
			    
--------------------------------------------------------------------------------
polar coordinates and camera,
we may express a position using two angles that represent a position on a sphere
like on the globe of the world the meridian and latitude. Once the direction
from the center of the unit-diameter sphere is determined, we multiply the
coordinates with r, the radius.
The euclidean coordinates are salvaged from the angles expressing the direction
(theta and phi) by using the trigonometric functions sin and cos in sequence
for we first need to get the length of the y-axis as shortened by cos(theta),
then sin(theta) multiplied with cos(phi) yields the 'x', and the complementary
sin(theta)*sin(phi) must be 'y'. The result is scaled using r, the radius.
The world's latitude and meridian may serve as an analogy but they differ a bit,
using lambda and phi as angles and starting at the equator.
The use of polar coordinates is useful for the camera, as we can change a single
angle and emulate a rotation about a reference point: as changing a single
angle makes the position move on a circle relative to a center point. In the
world-scene example that reference point was the camera-target, we simply
(glm:vec+ ..) added the resulting vector with the camera-target and now we
could move around(phi) up/down(theta) and towards/away(r) from a relative point!

Furthermore using the target point and the camera-point we can easily derive a look-at
direction (by normalizing the subtraction of these points!) then using a fiat up direction
(usually x=0 y=1 z=0) and the cross-product to yield perpendicular axis to build a
rotation matrix so as to place the world relative to the cameras view direction.  But
first we took the camera position and offset the world by putting it as the origin of
the coordinate system: translation matrix using the camera position negation!

Also note Mr. McKesson's interesting remark:
"'lines' in spherical geometries [as opposed to Euclidean geometry] are curves
when seen relative to Euclidean geometries."!!

--------------------------------------------------------------------------------
DEFCONSTANT and the compilation environment
;; because DEFUN: "Define a function at _top level_
(eval-when (:compile-toplevel) (defun foo () 2))


;; evaluates the expression (foo) in the compilation environment, and sets it
;; and defines the constant. Not so DEFUN! In the eyes of the compiler, it only
;; wants to inline the appearance of +qux+ henceforth with (foo). In the eyes
;; of a programmer, or API user, it is supposed however to show it's not to be changed.
;; Which does clash in a way with the inline use, as builder functions are not
;; evaluated in the compilation-environment!!

;; Furthermore DEFCONSTANT has problems with non number values etc. TODO
(defconstant +qux+ (foo))


;; defconstant expand:
;; (EVAL-WHEN (:COMPILE-TOPLEVEL :LOAD-TOPLEVEL :EXECUTE)
;;   (SB-C::%DEFCONSTANT '+QUX+ (FOO) 'NIL (SB-C:SOURCE-LOCATION)))

;; defun expand:
(PROGN
 (EVAL-WHEN (:COMPILE-TOPLEVEL) (SB-C:%COMPILER-DEFUN 'FOO 'NIL T))
 (EVAL-WHEN (:LOAD-TOPLEVEL :EXECUTE)
   (SB-IMPL::%DEFUN 'FOO
                    (SB-INT:NAMED-LAMBDA FOO
                        NIL
                      (BLOCK FOO 2))
                    NIL 'NIL (SB-C:SOURCE-LOCATION))))
--------------------------------------------------------------------------------
uniform buffer object UBO,
'global', across-all programs, uniforms, so we can set data that is shared, so far
read-only examples, by any number of OPENGL programs.

basic setup:

1. expose a handle to the uniform block called a 'uniform block index' in
this case (setf handle (%gl:get-uniform-block-index "system-area-pointer-string"))

2. Use that handle to associate the current program through it with a
'binding-index' which is a 'slot in the OpenGL context'

Build the uniform-buffer data somewhere, it must be stressed that data-wise it
is just an ordinary buffer object:
(setf *ubo* (first (gl:gen-buffers 1)))
(gl:bind-buffer :uniform-buffer *ubo*)
;; format the buffers data to expect data input (:steam-draw), and give it
;; a fixed size. The <null-pointer> tells OpenGL that we don't care about former
;; content of this buffer. For us it means we will be filling it with data in the
;; future
;; optimization:
;; (%gl:buffer-data :uniform-buffer <null-pointer> <size-of(mat4)> :stream-draw)

3. Fill (allocate) the buffer with data
 (gl:bind-buffer :uniform-buffer *ubo*)
 (gl:buffer-sub-data :uniform-buffer <gl-array> &optional: <offset> <buffer-offset>)
 ;; unbind: (gl:bind-buffer :uniform-buffer 0)

4. Finally provide the connection between the opengl-program's uniform-block and the
   uniform-buffer-object. Use the *ubo* handle and the 'binding-index':
   (%gl:bind-buffer-range :uniform-buffer <binding-index> *ubo* ...)
   

 KA-BAM the opengl program has them datasss, due to the this voodoo spell:

Data arrangement:

[data(gl-array)]->[uniform buffer object]-.
                                          v
          [binding to context via _uniform buffer binding points_]
                                          v
[program-obj]<-[uniform-block-index]<-----'
                                          '
[other program-obj]<-[...]<---------------'					  
                                          '
					(...)

Code arrangement:
For gl Program:
;; tell program where to find the uniform buffer:
(gl:uniform-block-binding <prog>
                          <uniform-block-index>
                          <BINDING-INDEX>)

For UBO:
 ;; bind uniform-buffer to the 'binding point'
 (gl:bind-buffer-range :uniform-buffer <BINDING-INDEX> *ubo* ...)

For OpenGL context:
 (defconstant +BINDING-INDEX+ 0) ; 0, 1, 2, 3, 4...

--------------------------------------------------------------------------------
GIMBAL LOCK,
the gimbal simulation in chapter-8/gimbal-lock.lisp uses the hierarchical model:
outer->middle->inner-gimbal->'ship'
'gimbal lock' appears when the outer and middle gimbal lie in the same plane
because of the axis-wise perpendicular arrangement of the gimbals the outer
and inner gimbal will be parallel -- rotate about the same axis.
Different explanation (hierarchical model):
each gimbal inherits an orientation from its predecessor gimbal with the
outer gimbal being the parent. Successors have the intrinsic property of always
being able to rotation-transform its children about a perpendicular axis from
its predecessor. A strange, bizarre, effect can occur: a higher(in terms of the
hierarchical model) gimbal can directly manipulate the rotation axis of its
successor gimbal. And because every gimbal is perpendicular to one another
it can actually 'fold' its successor axis INTO its PREDECESSOR axis.
Now this gimbal, in the middle, can watch its predecessor and successor revolve
about the same axis.. what a sick joke!
Another note:
every gimbal is always perpendicular to one another in the hierarchy, euclidean
space provides 3 possible axis, that's why for this binary-relation to be
always true 2-axis may suffice (two gimbals, next to each other, need each
a different axis).

But the main question remains: Why is this a big deal?
We started out by proposing a way to express an arbitrary orientation in euclidean space
using the three angles pitch,yaw and roll in order of the gimbal-lock.lisp hierarchical
model: x->y->z Note how we can't avoid using the 'hierarchical' model: If we wanted to
move about an axis without manipulating another we would get non-perpendicular planes =>
distortion of our object would occur

Further excursion:
Before we can talk about gimbal lock we need to understand:
- axis have a HIERARCHY, higher ups TRANSFORM lower axis
- axis are at 0-degree all perpendicular to one another

There is a given hierarchy of axes, yes Euler angle can represent any orientation in
euclidean space and finally, my personal last insight needed, gimbal lock is a problem of
continuous movement. If we try to orient an object, when it is already in a certain
orientation, such as, best example, in gimbal lock, we can't perform straight forward
transformation because we lack a degree of freedom. We need to backtrack the rotation
where the degree of freedom is restored and approach the orientation we wanted all over
again. This backtracking is what is bad for animators as between keyframes there will be
discontinuous or deviant movement from the path of 'apparent least resistance'.

To directly feel the problem while facing it: set up the simulation of gimbal-lock.lisp
into a gimbal lock. Notice how you can perform only two kinds of rotations directly:
rotate it about two axes (the middle one, and the two aligned outer and inner gimbal axes)
now try to perform the missing rotation in that gimbal lock position. You will now notice
how you need to 'backtrack' the way you got into gimbal lock to perform the previously
missing rotation.

With this you can witness, first hand, the problem of gimbal lock.

You may try to rotate the outer gimbal 90-degrees to gain the missing rotation, but again
you will miss a degree of freedom. With this a new observation occurs: if you can't perform
all three possible rotations at a given time you're in gimbal lock and have to transform
the object in a non-straightforward way to reach an orientation. This non-straightforward
transformation is especially problematic if we want to animate a fluent movement and
the object changes orientation in a quirky manner - we got gimbal lock'd!


Anecdote [from internet],
spherical coordinates - poorly defined nadir/zenith
You meet an Eskimo at the northpole and ask for direction to Europe. The Eskimo cheerfully
tells you to go south. You look at your compass. The needle is spinning like crazy while
whispering: any azimuth is at zenith, this be gimbal lock.

The Eskimo breaks your focus shouting:"Where's your fourth Gimbal NOW?"  You fall out of
bed, it was just a dream. You want to look at the clock behind you at the window so you
look up at the ceiling, but your neck can only get you so far, so you turn around
180-degree still facing straight up. As you lower your neck by a certain altitude you see
the clock.  You wonder how whatever time it is, the trigger's of the clock (it is an
analogue clock) always remain within the set of 12-hours. Actually adding 0-seconds to the
time, doesn't change the time at all and if you add x-minutes with y-seconds or the other
way around doesn't matter either, it still will show the same time.  For some reason you
have to think of donuts and coffee, so you get up and go to the kitchen.  

As you pass the many curves of your corridor, you imagine being the shadow of a fourth
dimension that can move freely in the world.. anyway.  You finally get your mug of coffee,
after some brewin. There is a small cookie in it. As you tilt the mug the cookie remains
in its orientation - no discontinuous jumps between keyframes (lol).

You think to yourself: If only there was an orientation possible that has the properties
of the clock and which is as efficient as my compass. You chuckle, "that could only
be possible in theory, you know, in a simulation".

--------------------------------------------------------------------------------
Orthonormal,
once you polish up your English you'll know: perpendicular=orthogonal=at right angle to one
another. And then you have an intuitively understandable term:
A coordinate system, which a matrix represents, with orthogonal basis vectors of unit length
(normalized) is called /orthonormal/!

--------------------------------------------------------------------------------
Quaternions,
As introduced so far a 4d-vector (x,y,z,w) with x,y,z being the 'vector part' and w being
the 'scalar part'. We represent an axis, with the vector part, and a rotation around this axis
with the scalar part. This solves the problem of Euler Angle's gimbal lock, also matrix orientation
representation whereby we'd have to orthonormalize the matrix representing an orientation on each
transformation lest accumulating rounding imprecision.

Quaternion multiplication,
two orientation quaternions can be multiplied -- we get the intuitive composite
orientation.  The multiplication is non-intuitive: it looks rather arbitrary and is
therefore hard to remember. It is non-commutative and associative though, just like matrix
multiplication.

The main difference between matrix and quaternion representation is that they're more
efficient, for example they're  easy to keep  normalized -- simply applying vector normalization.
This keeps them in unit-vector form.

Note how it is actually difficult to keep a matrix orthonormalized, which:
1. must be done on each frame of an animation to dodge rounding problems
2. involves the three basis vectors
3. once they're normalized they actually need to be all orthogonal to one another, but where
   to put a reference basis vector so its position is right and hence the relative orthogonal
   ones too? -- according to Mr. McKesson this can be solved but we will do it with quaternions
   still for performance reasons.

Now we know how to composite quaternions, hence can facilitate multiple changing orientations. But
our product so far of a quaternion multiplication is still just a quaternion we need something we
know how to work with:

Quaternion to (transformation)Matrix,
given a quaternion: q = (x,y,z,w) and a component being represented q.x,..., q.w then we
get a transformation matrix like so:

[1-2*q.y*q.y-2q.z*q.z    2*q.x*q.y-2*q.w*q.z    2q.x*q.z+2q.y*q.w 0]
[  2*q.x*q.y+2q.w*q.z  1-2*q.x*q.x-2*q.z*q.z    2q.y*q.z-2q.w*q.x 0]
[  2*q.x*q.z-2q.w*q.y    2*q.y*q.z+2*q.w*q.x  1-2q.x*q.x-2q.y*q.y 0]
[           0                     0                     0         1]

I wouldn't be surprised if there is a typo in there. Anyway, it looks similar to creating
a matrix from angle/axis rotation. But since we're dealing with quaternions I think it is
sensible to drop the aspiration of trying to understand them intuitively. I only barely
understood matrix multiplication intuitively, with some fuzzy loopholes here and
there. But those are 3D, here we're dealing with topological wizardry of 3-spheres --
4-dimensional geometrical representation, a covering map that provides 'homeomorphism'. I
don't know if it is worth trying to learn that stuff, if I won't encounter it again I
shouldn't bother as it may be too special to be even worthwhile to know when tackling
problems related to graphics programming.

Putting it all together intutively:
The usage of quaternions is to be understood as a more efficient solution to the gimbal lock problem.
The gimbal lock problem arises from trying to represent orientation as a sequence of hierarchical
rotations. This is space efficient, three angles is all you need. But interlocking planes (gimbals)
may drop the degree of freedom.
What we want is a transformation that changes the current orientation relatively, so we can always
perform a object relative pitch so that a rendered space-ship will always pitch instead of some
absolute orientation. This can be solved using a transformation matrix to represent the orientation
and then multiplying it with another to get this relative transformation.
This is indeed a solution, but there is a problem to it, it is inefficient:
1. We need to store a whole matrix,
2. as we apply multiple transforms per frame rendered, expecting 30 frames per second we can see
   that rounding problems - floating point imprecision - may accumulate.
3. To fight these imprecisions we want to keep the matrix othonormal, a costly operation: every
   base-vector needs to stay normalized and, a bit more problematic, the base-vectors must stay
   perpendicular to one another.

This can be certainly done but there is a much more efficient solution that allows easy normalization
and less values to represent the orientaion:

                                         The Quaternions

1. A quaternion can represent a transformation matrix (rotation about a arbitrary axis by an arbitrary
angle), we can easily transform a quaternion into a transformation matrix.
2. We don't need to multiply the resulting matrix, the underlying quaternions, of each matrix, can also
   be multiplied with each other - Quaternion Multiplication - and the result can be, on each frame,
   transformed back into a transformation matrix.
3. The quternion having a scalar-part (the angle of rotation) and a vector-part (axis to be rotated about)
   can be easily normalized using simple vector-normalization.

Transformation matrix need multiple normalization, quaternion only one per transform.
Transformation matrix take up a lot of space, quaternion being a 4d-vector much less.
Transformation matrix multiplication is more expensive and involves more multiplicaiton in general
wheres quaternion multiplication involves much less.


The gimbal lock problem solution can be thus abstracted like so:

Input          current coordinate system     transformation                    new orientation (new current
               representation                                                  coordinate system)
               
point  ==>     transformation-matrix   ==>   matrix multiplication      ==>    transformation-matrix

                    ^                                                                  ^
		    |                                                                  |
          <quaternions can be always turned back into a transformation matrix, at any of these two points>
		    |                                                                  |

OR:              quaternion		     quaternion multiplication         matrix-casting a composite
	                                     the result being a                quaternion
					     composite quaterion

GLSL: note that GLSL doesn't provide types for quaternions or any quaternion arithmetic.

					     
--------------------------------------------------------------------------------
Inversion,
We wanted to apply an orientation offset to our points, and so far we've been doing
this using the same equation, or order of transformation that is hardcoded in
our shader program:
1. model-space (our cute ship model)
2. world-space (where should the ship be)
3. camera-space (from where we look at the world scene)
(4.) projection divide

Or as a matrix transformation: C*O*p (dunno why the auther uses O for world space)

In the camera-relative.lisp example we're only allowed to change transformations
changing the C with a right-hand multiplication, this change is the R-matrix:
 C*R*O*p
but we wanted to perform transformation relative to our "eye" the camera
matrix, so what we really wanted was:
R*C*O*p

This restriction, of using the R in place C*R*O*p but still achive camera-relative
orientation: R*C*O*p can be solved by learning about inversion!

Inversions have this cute property that they can be yielded from any matrix (I think
there are mathematical exception but as far as graphics orientation matricies are
concerned we always can yield those). This inversion matrix of a matrix M is called
M^-1 and if we multiply them _in any order_ we get the IDENTITY matrix:
 M*M^-1 = Identity-matrix = M^-1*M

hence, because an identity matrix is the neutral element of matrix multiplicaiton, in our
example C*R*O*p the 'R' can "kill" C by turning into a C^-1:

C*C^-1*O*p = O*p (yay)

or, furthermore, we can turn 'R' into a amalgam of C^-1 and C and we "kill" our R:
C*C^-1*C*O*p   = C*O*p
  ^^^^^^= our R

and to finally make it sensible we sandwich between our amalgam C^-1*C a real
transformation we care about, namely the R to begin with, hence new-R = C^-1*R*C
and put it in the equation:
  C*new-R*O*p   turns into:
  C*C^-1*R*C*O*p
  ^^^^^^death
         R*C*O*p

and boooom we have our desired camera-relative transformation, without hardcoding the
order of multiplications for the whole formula.

From an intuitive perspective an inversion matrix can kill its neighbors left and right,
and therefore build a bridge to different relative transformations.

In the code example in camera-relative.lisp we could put a "C" inside the R transform
because we had access to the current camera-matrix through functions that could
readily compute it (resolve-cam-position) and (calc-look-at-matrix ...).

--------------------------------------------------------------------------------
Quaternion interpolation,
another advantage of using quaternions. We can perform linear interpolation on the
components of a unit vector, as long as we normalize the result and we get a
meaningful interpolation as showcased by 8-chapter/interpolation.lisp.

However we notice that the interpolation sometimes significantly speeds up and slows down,
this is due to the nature of component-wise interpolation as it is geometrically
unconscious of the other components. Not so with:

Spherical linear interpolation or SLERP,
here we interpolate a vector along the same plane to another vector. This is accomplished
by using the dot product, which yields the angle of two vectors.
Now we use the interpolation alpha [0, 1] to interpolate over this angle:
alpha=1 -> full angle   alpha=0.5 -> half angle.

The transition is performed by shrinking the source vector (v0) with a COS of
the angle between the two while growing the destination vector slowly:

v0*cos(alpha*angle)  and v1*sin(alpha*angle)

now all we need to do, to get a meaningful interpolation vector, is to add them together!

v0*cos(alpha*angle) + v1*sin(alpha*angle)

It is now possible to imagine how one vector shrinks and the other one grows at its tip
(remember the meaning of vector addition) the resulting vector is the one on the
"angle plane" between the two, and we get a meaningful steady interpolation of
orientations!

Note how we don't need to normalize the result if the inputs are unit-quaternions, in the
case of SLERP, because if the two trigonometric brothers Cos and Sin work together they
take and give in equal amounts, so the result is always in equilibrium - no normalization
needed. :)
--------------------------------------------------------------------------------
Lighting,
in arc we will only focus on _photorealism_ as opposed to non-photorealism (NPR).
A prominent example of NPR is the cartoon style of "cell shading" others include
paintbrush effects, pencil-sketch and other things.

lighting model,
is the algorithm used that determines how a surface interacts with light.

;;TODO: rewrite for brevity and punch abstractions ?

light sources,
light emitting objects: the sun, a lamp, fireflies      [direct]
OR
A surface that REFLECTS light.                          [indirect]

Simplification:
Surface,
can only do two things with light: reflect or absorb. That's where some real-life
principles apply: a surface appears of a certain color if it absorbs every other color,
and reflects only that certain color.

Note for a surface to be, say, of red color, the color red must be present in the _light
source_ shining upon it. On the other hand a surface that is blue but is shone upon by
red light will have no color to reflect but only color to absorb. The fiat "color" for no
reflection is black. Just like in real life wearing a black shirt, that absorbs "all" the
colors of the visible spectrum will also get hot faster, on the atomic level the atoms
actually "capture" the light and the whole system gets hot - energy input takes place.

RGB,
we only care about three "wavelengths" the colors red, green and blue.

intensitiy of light and angle of incident,
a light source emits with a certain light intensity. That light can reach a surface at a
certain angle, the angle of incident. The intensity depends then on the area that is
covered by that light the steeper the angle the bigger the area covered the more "light is
distributed" and hence each point receives LESS light.
Following this logic: a small room can be more brightly lit,_AT EACH POINT_, than a big
one.
Now don't you forget the surface's absorbing characteristic, which then reflects the
incoming light.

our new words in a causation chain:
"->" = light intensity weakens or stays the same at point
light source -> angle of incident -> absorbing characteristic of surface -> reflection

The light source, surface absorption and reflected light can be described as RGB colors
on the range [0,1] !!

A lighting model is a function of all these parameters! There are a lot of parameters to
come in the future like these:

/Diffuse lighting/,
particualr surface/light interaction where the light is reflected in all directions
("many angles") instead like as if of a perfect mirror. This is another surface property

Lambertian reflectance,
a model of diffusion lighting in which a surface will reflect the light equally in all
directions.
Its equation, where R=reflected color, D=diffusion surface absorption, I=light intensity
and theta=angle of incidence
   R = D*I*cos(theta);  // note cos(0-degree) = 1.0 aka brightest also negative values
                        // will be clamped

--------------------------------------------------------------------------------
How to compute: angle of incidence, and where to perform the lighting computations

surface normal,
traingles have this nice property that all its vertices occupy the same plane and
hence its side has a definite direction it can face: the _surface normal_ or just
_NORMAL_! Every point along the surface of single triangle has the same geometric
normal.
But the problem is, we don't want those, as our object's are not to show this property of
being fundamentally triangles and hence have very faceted lighting. This can be solved by
assigning to each vertex the normal it /would/ have to be an approximation of the object
it tries to approximate as a part of a polygon. Yes, each vertex will gets this additional
"normal" data.

The vertex will now contain the data: position, color and normal!

Now to draw what is within the vertices we will need the normals for all those points too,
right? Yep, we and get those by interpolation from the vertices surrounding them!
This simple and efficient computation has a name: 

Gouraud Shading,
interpolating the result of Lambertian reflectance at the vectors (which contain the
normals as meta data) of a vector over the surface of the triangle. This approach to
lighting over the surface of a triangle is very efficient - it's just color interpolation
over the triangle - and was in fact the preferred method as it is also a fairly decent
approximation. Its Achilles heel is that it is used for diffusion lighting and that, for
example, modern games just don't use diffusion lighting anymore and it becomes noticeably
inaccurate in other lighting models.

The lambertian reflectance formula can now be filled:
R = D*I*cos(theta)   ; theta = dot(normal, light-source-direction);

D given by the vertex data, as for the theta, the angle of incidence, we take the normal
from the vertex data and calculate the dot product of the light-source direction.  The
light intensity is a property of light, and while the light direction can be a RELATION
between the surface point position and the surface point, we can allow another
simplification:

The light source direction can be simply a SINGLE direction for every point on the scene
this simplification makes sense when we consider the sun in relation to the earth: the
sun is so far away that on earth it appears that its "sun ray direction vectors" are all
the same at each point. This approach is called: DIRECTIONAL LIGHT SOURCE

This makes sense in a scene where the relations are quite different: a small lamp on the
desk illuminating some fruits. Here the direction of the light relative to each surface
points of the fruit is much different. Analogy: the farther two adjacent lines meet at
their ends the smaller the angle between them, with great distance they can even be
considered to be parallel (same angle relative to the surface they shine on, the same
direction relative to each point on the scene's surfaces).

--------------------------------------------------------------------------------
Transforming Normals Non-Uniformely,
Normals may not represent the object once non-uniform transformations take place,
such as: scaling non-uniformely :scale 1.0 2.0 0.2, or other :rotate 45.0 etc.
combinations.

This can be solved by applying an _inverse-transpose_ on the matrix before passing it for
normals to be transformed by. The explanation will be omittet, not only is it somewhat
hard it also is very rarely used, that's because non-uniform scaling is rarely used! But
for the arcsynthesis tutorial we will use non-uniform scales to build complex constructs
from simple primitives, that's why they're briefly introduced. It should be noted though
as I have read this myself a few times that any number of arbitrary transformations to
a matrix M_ can be expressed as a scale-matrix sandwiched by two rotation matrices:
M_ = R1*S1*R2 !!

The process is just a (transpose (inverse norm-matrix)) and the high-level reason for it
is that normals will now be represented properly after any amount of non-uniform scaling
rotations and other transformations combined.
(see scale-and-lighting.lisp for an interactive graphics example)

It is still interesting how these operation is derived, just so as to see how matrix
equations can be solved to yield a simple final operation. Like mathamaticians yearn
for short equations, so does the graphics programmer, as the GPU in question will
have to execute them quite a lot. The bottleneck then is widened through matrix
arithmetic. Finally note that this operation is still an expensive one, and should be
avoided.
--------------------------------------------------------------------------------
Global illumination,
Our cylinder in basic-lighting.lisp has a pitch black back side. This is not realistic,
as anyone can observe themselves: if the sun of the light doesn't hit an object directly
doesn't mean it will be pitch black.

This is due to the property of object reflecting light in all directions thereby the
shadows are only shadows, not pitch black relms, because light from other object reaches
it. This property of the world, a phenomenon, is called: _interreflection_!

A lighting model that employs interreflection is said to handle _global illumination_!

In contrast: Modelling light only from a light-emitting surface is called:
_local illumination_ or _direction illumination_! Our previous example, hence, employed
direction illumination.

But doing actuall global illuminatino is very hard, we will instead use a time-tested
frugal technique called:

Ambient lighting,
this one is simple: we assume that on each point in the scene there is a certain
level of lightintensity always present. This is like a crude approximation to the fact
that light comes from everywhere to some degree. Note that for this model we don't even
need a angle of incidence!

--------------------------------------------------------------------------------
Mesh Topology,
because each vertex attribute shares one index namespace --each value has a unique index,
we want to combine position vertex-attributes with normal vertex-attributes we can't store
them in two different arrays and map in a meaningful way, say, to represent a cube.
Instead we must have a unique index for the combination of the two:
(pos, normal) so that on each execution of e.g. the vertex shader the "layout... vec3
pos... and layout vec3 normal" store a corresponding value. This also means that since
many positions share the same normal (for example all position of a single face) we need
to provide the data each time the position is needed in a vertex array. This providing,
association of meaningful data to a single shader call or vertex attributes is called a
mesh's topology, because it is this topology of interconnectedness that is needed to draw
a meaningful object like a cube, cylinder etc.

Again this wasteful creation of unique combination of repetitive data (each face's
position has the same normal, or even the same color) is needed because we only have a
single index array to draw a single index value from. See the "layouts" from the
gl:enable-vertex-attrib-array etc. calls. And this single index needs to associate all
the unique combinations with eachoter (position, normal, color) to draw the desired
graphic from the mesh data.

--------------------------------------------------------------------------------
_point light source_,
the light source is represented by a position in the world, hence the light direction
is not contant anymore but relative to this position and will be computed now for
each point of interest.

Gouroud shading and point light source,
linear interpolation over the surface of a triangle when the lighting model is only
applied to the normals, the corners of the triangle, is called gouroud shading. This
approach is failling us when using point light sources. As can be seen in
vertex-point-lighting.lisp when the light is close to the ground plane it isn't
lit lineary intensive from the point of the light source to its corners because
the corners of the ground plane apply the lighting model, AT A STEEP ANGLE, and
then interpolate this value over the surface of the triangle. What we get
is steep values from the gound plane corners interpolated => not very bright colors.

To solve this problem we need:

Fragment lighting,
Here we simple apply the lighting model to each fragment of a triangle. Instead
of interpolating the result of the model from the three vertices of a triangle we
apply it to every single fragment by simply putting the formula into the fragment
shader. The fragment shaders' modus operadi is to be executed for each fragment
withing a given triangle as input, interpolating all the position, color and normals
for each fragment.
These values can be placed into the lighting model formula directly but the
light direction must be computed:
light-source-position - fragment-direction = direction-from-fragment-to-light

Now the angle of incidence can be computed, maried with cosine, clamped etc. and
we get a more photorealistic behaving lighting model!


--------------------------------------------------------------------------------
Light Attenuation,
light intensity on a surface ought to weaken with the distance to the light source.

Light attenuation is a well-understood phenonmenon: "light intensity weakens with the
inverse square of the distance between light source and surface of interest"

The corresponding formula is:          I           <- (light) intensity
                               ------------------
                                (1.0 + k * r^2)    <- r = distance: from light to surface

Where 'k' is a special constant that must be precomputed for this formula to be used.
The meaning of 'k' shall be this:

k = 1 / hd^2      where hd is the "half-distance" or the distance at which the light
                  intensity shall be cut in two.

example: We want light to be half as strong at a distance to the surface of 4.0
k = 1 / 4.0^2
k = 1 / 16
--> put k into above formula:
        I                       I               
------------------  =>	------------------   now taking an actuall distance of r = 4.0:
 (1.0 + k * r^2)  	 (1.0 + 1/16 * r^2)

        I                      I                   I            I
------------------  =>  ------------------ =>  ---------  =>  -----  indeed, light intensity
 (1.0 + 1/16 * 4^2)     (1.0 + 1/16 * 16)      (1.0 + 1)       2.0   is cut in half at a
                                                                     distance of r = 4.0 !


Below is a cl function that takes a, scalar, half-distance, plugs it into the "k = 1/hd^2"
equation and returns an attentuation light computing function. Use this function to get
a feeling of how a "inverse square" distance influences light intensity


(defun al (k)
  "take the 'half-intensity-distance':k and return a function
that will take the actual distance and compute the intensity.
Where if r = half-intensity-distance then r->0.5!"
  (lambda (r)
    (/ 1.0
       (+ 1.0 (* (/ 1 (expt k 2)) (* r r))))))

> (funcall (al 4.0) 0.0) ==> 1.0
> (funcall (al 4.0) 1.0) ==> 0.9411765
> (funcall (al 4.0) 2.0) ==> 0.8
> (funcall (al 4.0) 3.0) ==> 0.64
> (funcall (al 4.0) 4.0) ==> 0.5

Newtons law of universal gravitation also dictates such a distance relation between
bodies, but, in its case concerning the gravitational pull on each other "increasing with
the inverse square" of the distance between them -- explaining the planets ellipsoid
trajectories around the sun.
Indeed the inverse square distance relation is recurring among various phenomena:
electric, magnetic, light, sound, and radiation (wikipedia: "inverse-square law").

--------------------------------------------------------------------------------
The diffuse lighting model holds true for very smooth surfaces, but only a few
materials in reality have this property. We need a refined model that includes

specular reflection,
were shiny surfaces will reflect light more strongly in the opposite direction from the
angle of incidence! Which is the angle between the surface normal and the incoming
light directoin.
A mirror has perfect specular reflection!

specular highlights,
though light bounces all over the place and refracts in the air we will only model
specular reflection from direct illumination --the light that was received
directly from the light source (not indirectly from a reflecting surface). An
area on the surface that is bright due to this direct illumination is a specular
highlight. We won't care about indirect light because in reality it also only
negligibly contributes to the overall illumination.

Microfacets,
this is a model of for surfaces that basically says that a surface roughness is explained
by it having tiny mirrors that point in different direction. The rougher the surface
the more they point in different direction.

This explains why a rough surface may reflect light more broadly (more different
direction) but also (!) less intensely because the light can't focus well in a particular
direction.

Conversely, a smoother surface reflects the light less broadly (microfacets point
more in the same direction) but therefore at least more strongly.

Analogy: a perfectly smooth suface is a mirror, in which you can see the sun reflected
perfectly. Whilst on say a stone wall, an intrinsically rough surface, you just see
a bright overall surface and note how the sun can be glaring indirectly through a mirror
but it is not glaring when you look at a wall with the sun in your back.

angle of view, or, viewing angle, 
this is the angle at which we look at a particular point on a surface. If it is the same
as the anlge of incidence then we should witness the strongest specular highlight at it:
This is the angle at which the light would reflect into our eye "directly" from the
surface -- the shortest distance.
I guess for now we will model specular highlights lesser the more the surface view angle
and angle of incidence deviate plus some factor for smooth surface (sharp decrease) and
for rough surfaces (slow decrease but overall weaker factor)!

microfacets distribution,
while we say the rougher the surface the more perfect little mirrors, the microfacets,
point in more different direction it is up to the specular illumination model how
to model this distribution of microfacets on a surface and how they may contribute
to the _average surface normal_ down to the pixel level!

It is important to remember that while the average surface normal may not reflect light
directly into the eye (angle of incidence != viewing angle) some microfacets at the point
of interest may still reflect some light into the eye.

--------------------------------------------------------------------------------
Phong model,
just like Lambertian reflactance is the simplest model of diffuse lighting, the
Phong model is the simplest model of specular illumination!

Like anticipated, we take the deviation between the perfect reflection angle and
the viewing angle: (V*R) <- this will get us the cos(theta) angle between the two
if they're identical cos(0)=1.0. But we're not done yet:

(V*R)^s         (This term will be multiplied with the intensity in our lighting
                 model equation!)

V= view direction; R = perfect reflection vector and finally s
S will be a value between [0, infinity) and will be our roughness/smoothness factor
and could thus be attached to the fragment of interest as an intrinsic value, a
property of the material if you like.

Note that the angle between V*R, theta can due to clamped cos() only be between
0 and 1, thus raising it to the s-power will mean a sharper decrease in brightness.
Hence the higher S, the more a deviation will be punished with less brightness.
This is the property we want to attribute to smooth surfaces.

high S = smooth surface, lower S = rough surface

Finally raising a value to a power makes for an exponential curve, hence we
get a nice non-linear decrease in brightness which is handy for faking the
smooth/rough surface illumination.

The Phong model has two major drawbacks: it doesn't take the statistical overall
distribution and orientations of microfacets into account, as it only derives
its values from the view-direciton and the reflection-direction. But even worse
is that because this value is dirived from these two direction it completely
hits a brick wall when the angle between the two exceeds 90-degree, as can
be seen in the phong-lighting.lisp tutorial forming a clear cut fringe around
the specular hightlight when the surface roughness high.

The following model can solve this problem:

--------------------------------------------------------------------------------
Blinn-Phong specular model or just Blinn specular model,
The problem, again, is that at the reflection and view direction vector exceed
90-degree's the phong model breaks down, so we will, speaking at a high level, simply map
the whole range of possible degrees 180 to 90.

Why 180 degree? The extreme case is that we view the scene with the sun in our back down a
flat surface. Here the reflection will point ideally exactly away from us: 180-degrees.

This mapping will be accomplished using the:

half-angle vector,
This is just the direction vector between two vectors that cuts them, angle-wise, directly
in the middle. So two vectors with a 90-degree angle have a half-angle vector between them
at 45-degrees.

Or: if we add two vectors together we get their half-angle vector!

Now it should be possible to imagine that if the light and reflection vector perfectly
allign that the half-angle between the light-source and them will be also perfectly
alligned with the surface-normal.

And this will be the allignment we will be testing for with the Blinn-Phong specular
model: the deviation of the half-angle vector of the light source and the view-direction
from the surface-normal. Yes, we don't need the reflection-direction anymore. This deviation
will then be raised by our surface roughness/smoothness factor, s:

         H=light-directon+view-direction aka _half-vector_
(H*N)^s  N=surface-normal s=surface-roughness

--------------------------------------------------------------------------------
Gaussian specular model,
finally we will actually try to model the distribution of microfacets over a surface.
We try to model realistic specular lighting by deriving the dozens of microfacets
orientation on the surface through the powerful tool of statistics.
The method of statistics we will use is the most simple and eponymous /Gaussian
distribution/ (also: "normal distribution) which in essence is a bell-shaped curve
whose shape can be either flat and broad (roughness) or tall and thin (smooth)
whith the added benefit of having the curve distribute values along normals. The
following function is a simplified version of the Gaussian Term:

e^-(a/m)^2   e = euler number; a = angle between the surface-normal and the half-angle
                                   between the view-direction and the light direction
				   but, this time not the cos, but the actuall radiant.
				   yielded using acos(dot(half-angle,surface-normal))!
				   
and 'm' again just the surface smoothness/roughness factor, this time though, the
higher the value the rougher the surface (the flatter the bell shaped curve). Note
how 'm' is 
Also, m, may only range from (0, 1] for the model to not break.

--------------------------------------------------------------------------------
Specular model performance,
while Phong needs to compute the reflection direction and Blinn only needs the
half-vector, it is negligible performance-wise.
The more interesting fact is that graphics hardware can perform the inversqrt,
nowadays (!), about as fast as the dot-product itself.

Both these operations, by the way, are always used in normalization (!!):
1. first we dot-product with the vector itself
2. then we inverse-sqrt multiply with it!
(no component-wise Pythagoras school-algebra)

But if we look at the Gaussian specular model we find ACOS in the algorithm,
the inverse cosine, which is a notorious performance killer.

--------------------------------------------------------------------------------

material properties,
properties of a surface, such as roughness/smoothness factor, the color used when
diffusion takes place and the color used when specular lighting takes place etc. This
takes into the lighting model the surface itself, as light does behave differently for
different surfaces. According to John Carmack the accurate choice of material properties
is where the rasterization photorealism approach can still do a lot. This is an issue that
professional modelers have to take into account when creating models.

light clipping, or light clampping,
when the light intensity falls outside the [0,1] range. Usually in the positive range.
It looks somewhat interesting and might have just inspired some effects in various
media.

--------------------------------------------------------------------------------


//code porting skip takes place here//
//the following notes are now skimming much more parts of the tutorial//


--------------------------------------------------------------------------------
HDR - high dynamic range lighting,
we emulate the behaviour of our iris: it contracts in bright areas and expands as the
area gets less bright. Hence we take the overall illumination into account and either
filter out some more or some less light.

--------------------------------------------------------------------------------
gamma correction,
very intersting, since the times of CRT TVs the color mapping relationship
between light intensity and color is non-linear. The modern screens even though
all don't use CRT still take this assumption when displaying colors. We need to
counter this assumption by performing gamma correction which in effect simply
raises the, for example, RGB color to the 1/gamma power where gamma is usually
2.2:

  linear-RGB^1/2.2 = gamma corrected color!

A quick and dirty solution might be to simply perform the sqrt(linear-rgb) as
sqrt is close enough to the root of radix 2.2

See the curve in the tutorial section "Linearity and Gamma" to see how this changes
the color mapping, how it was behaving until now "fully half of the linearRGB range
is shoved into the bottom one-fifth of the available light intensity.". Applying
this correction in our shaders makes the scene colors much more realistic, brighter
and also imporves the shadow detail considerably.

The CRT gamma-correction issue is due to cathode rays increasing brightness with
higher voltage, but in a non-linear way. To counter this issue the images send
to the TVs via satallites where sending the inverse of this voltage/brightness
relationship, thereby canceling out this non-linearity! The reason modern monitors
still take this into account is because like the TV-station sending gamma-corrected
images all sources: media, cameras etc. had to store the images gamma-corrected.
This legacy today in the form of DVD bluerays etc. forced the new generation of
monitors to adapt to this forced image property!

Since we de-linearlize the images to make them the "anti-wave" curve to the CRT
voltage-brighness wave to help them cancel out into a linear one, it is called
_gamma correction_.

 Attention! 
This means whenever we draw something directly to the screen, without going to
our "gamma corrected" shaders, we can't forget to apply this gamma correction!
(gl:clear-color ..) comes to mind.


inverse-square attenuation,
finally the reason why inverse-square attenuation wasn't realistic previously
was precisely because the monitor expecting gamma corrected images in the first
place!!
--------------------------------------------------------------------------------


//skipped some stuff here//


--------------------------------------------------------------------------------

ray,
a ray is a direction and a position. It contains infinite position bound
by its position origin and extends into inifity in its direction. The most
beautiful thing about a ray is the equation used to express the points within
this bound inifinite, well, ray: the..

Ray Equation:

P(t) = Dt + O

O = origin point of th ray
D = directino of the ray
where P(t) yields the position of points 

The beatiful thing is that this is very easy to visualize: We simply multiply
the direction starting at the origin O by the scalar 't', and we get the
resulting vector to point at any point along the ray!

--------------------------------------------------------------------------------
interleaving,
arranging vertex and their attributes next to each other in a datastructure is called
interleaving. _This data arrangement_ (!) should be used where possible. Trying to lay out the
datastructure in memory the c-struct comes to mind with two fields

a vec-type field, and a some attribut, like arc's example:

struct VertexData
{
    glm::vec3 cameraPosition;
    float sphereRadius;
};

--------------------------------------------------------------------------------
geometry shader,
a new shader stage! It is entirely optional and takes place between the vertex and
fragment shader stage. It takes a primitive as inputs and returns a primitive as output. A
very vivid example would be minecraft blocks: to store them efficiently we could use a
point, then put the point in the geometry shader and make it return a cube primitive!

This is why a better name for gemoetry shader would be "primitive shader"!

--------------------------------------------------------------------------------
Textures,  ;;TODO rewrite
a texture is a look-up table; an array. We start into this big topic with the big
disillusion: textures are _not_ pictures.

First problem is one of in which set or scope we're talking about. Because texture
can be pictures of something. Second, OpenGL reinforces that notion by calling
data in textures "colors" and many functions dealing with textures contain the
word "image"!

texture,
is an object containing 1..n arrays of varying dimensions. The storeage of a texture
is owned by opengl just like with buffer objects' storage.

images,
the arrays within a texture are called images!

texture type,
textures have a texture type defining, among others, the number of dimensions of
the images (remember images are arrays; arrays can have varying dimensions).

--------------------------------------------------------------------------------
We want to associate a texture with locations on the surface of an object. We
enter the problem of interpolation where perspective projection is non-linear
that is points move away from each other at a non-linear pace, namely a
logarithmic one (by orders of mulitplication, not by orders of units/addition).

There is a remedy to this:

_/perspective-correct interpolation/_,
we are divulged the final reason why we use clip space coordinates with their
w-component, which so far was used to perform the perspective divide, an operation
that is non-linear and hence not be accomplished with matrix multiplication.
We may be using a projection matrix, but the magic of perspective divide is
multiplying the positions with the 'w' component being -1, and projecting the
position down on the "screen" - or retina to be more imaginative.
While the 'w'-component seems to be the critical part implementation wise, as
far as we're concerned it is just changing the qualified of output variables
from "smooth" (= perspective correct interpolation) to "noperspective".
Finally the author even hints that window space interpolation is rather rare.

--------------------------------------------------------------------------------
_texture mapping_,
so far we've used shaders to lookup returns value for expensive computation
(e.g. gaussian term), based on the function input alone. 
In this case we can use it to vary the surface material property "down to the
texel" were we previously would assign a material property to only a vertex and
interpolte it over the surface of a triangle. 
For this to work we need to associate a texel with a point on a triangle, this
is called texture mapping, and grants much finer granulation of material
properties for point a surface.


--------------------------------------------------------------------------------
normalized integer,
dealing with floats in opengl oftentimes only includes the value on the range
[0,1] but to store a single float 4-bytes are needed. The idea of a normalized
integer is to map the whole range of floats from 0 to 1.0 to the to the range
of a unsigned byte, which has the range which can represent 256 distinct values
and only needs a single byte to be stored.

So how do we map a value like 0.52 to an unsigned byte?
Simple: 0.52 * 255 = 132 (rounded).

OpenGL will, provided the proper information via gl:tex-image* automatically
convert the unsigned byte (represented by the opengl type enum: GLubyte)
to a float. How? 
Simple 132 / 255 = 0.52 (rounding)

And this is how we compressed memory from 4 bytes (float) down to 1 byte (GLubyte)!

Note: This saves a lot of memory and when it comes to textures it oftentimes
saving memory yields improved performance.
